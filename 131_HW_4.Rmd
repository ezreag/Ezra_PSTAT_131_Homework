---
title: "PSTAT 131 HW 4"
author: "Ezra Aguimatang"
date: '2022-11-06'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```
```{r, include=FALSE}
required_packages <- c("tidyverse", "tidymodels", "ggplot2", "rsample", "yardstick", "corrr", "corrplot", "ISLR", "ISLR2", "tune", "dials")
lapply(required_packages, require, character.only=TRUE)
```

## Resampling

For this assignment, we will continue working with part of a [Kaggle data set](https://www.kaggle.com/c/titanic/overview) that was the subject of a machine learning competition and is often used for practicing ML models. The goal is classification; specifically, to predict which passengers would survive the [Titanic shipwreck](https://en.wikipedia.org/wiki/Titanic).

![Fig. 1: RMS Titanic departing Southampton on April 10, 1912.](131_HW_4_Images/RMS_Titanic.jpg){width="363"}

Load the data from `data/titanic.csv` into *R* and familiarize yourself with the variables it contains using the codebook (`131_HW_4_Data/titanic_codebook.txt`).

Notice that `survived` and `pclass` should be changed to factors. When changing `survived` to a factor, you may want to reorder the factor so that *"Yes"* is the first level.

Make sure you load the `tidyverse` and `tidymodels`!

*Remember that you'll need to set a seed at the beginning of the document to reproduce your results.*

Create a recipe for this dataset **identical** to the recipe you used in Homework 3.

```{r}
titanic_df <- read.csv("131_HW_4_Data/titanic.csv", header=TRUE)
head(titanic_df)
titanic_df$survived <- factor(titanic_df$survived, labels = c("Yes", "No"))
titanic_df$pclass <- factor(titanic_df$pclass)
str(titanic_df)
```

### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 

#### Solution 1:
```{r}
# sets the seed in order to reproduce results
set.seed(3435)

# creates an initial split of abalone data
titanic_split <- initial_split(titanic_df, prop=0.80, strata=survived)
titanic_split

# creates the training split using 80% of the data
titanic_train <- training(titanic_split)
head(titanic_train, n=5)

# creates the test split using the remaining 20% of the data
titanic_test <- testing(titanic_split)
head(titanic_test, n=5)

# verifies the correct number of observations
dim(titanic_train)
dim(titanic_test)

# creates recipe for data
recipe_titanic <- recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                         data=titanic_train) %>%
  step_impute_linear(age, impute_with = imp_vars(all_predictors())) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_interact(terms = ~ sex_male:fare + age:fare)
recipe_titanic %>% prep() %>% juice()
```
As we can see from the `dim()` function, we have the correct number of observations after using the split 80/20 for our training and testing data.

### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

#### Solution 2:
```{r}
titanic_folds <- vfold_cv(titanic_train, v=10)
titanic_folds
```

### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?

#### Solution 3:
```{r}
```
In question 3 we use the `vfold_cv()` function on our training data set. V-fold (also known as k-fold), is a method of cross validation that can be used alternatively to simply fitting and testing models on the entire training set (as we've done throughout this course thusfar). This function`vfold_cv()` creates a k-fold data set where we choose 10 folds, where 5 and 10 are common choices for the number of folds for this cross-validation method. This method essentially allows us to find the best value of degree that produces the closest fit, by randomly splitting out training data into k-1 folds and fitting our models to these split data sets in order to produce results with a much lower bias. If we were to rather use the entire training set, this would be called a "Validation Set Approach".

### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

#### Solution 4:
```{r}
# logistic regression
log_reg_model <- logistic_reg() %>%
  set_mode("classification") %>% 
  set_engine("glm")

log_wkflow <- workflow() %>% 
  add_model(log_reg_model) %>% 
  add_recipe(recipe_titanic)

# linear discriminant analysis
lda_model <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS")

lda_wkflow <- workflow() %>%
  add_model(lda_model) %>%
  add_recipe(recipe_titanic)

# quadratic discriminant analysis
qda_model <- discrim_quad() %>%
  set_mode("classification") %>%
  set_engine("MASS")

qda_wkflow <- workflow() %>%
  add_model(qda_model) %>%
  add_recipe(recipe_titanic)
```
As said before, since we test the folds on one left out fold, with a test of k-1 folds, we would be training each fold on 9 folds. But when fit to each fold overall we have 3 models meaning each model will be tested and fit on a total of 27 folds.

### Question 5

Fit each of the models created in Question 4 to the folded data.

**IMPORTANT:** *Some models may take a while to run – anywhere from 3 to 10 minutes. You should NOT re-run these models each time you knit. Instead, run them once, using an R script, and store your results; look into the use of [loading and saving](https://www.r-bloggers.com/2017/04/load-save-and-rda-files/). You should still include the code to run them when you knit, but set `eval = FALSE` in the code chunks.*

#### Solution 5:
```{r eval=TRUE}
# logistic regression fit
log_fit_resamples <- fit_resamples(log_wkflow, titanic_folds)

# linear discriminant analysis fit
lda_fit_resamples <- fit_resamples(lda_wkflow, titanic_folds)

# quadratic discriminant analysis fit
qda_fit_resamples <- fit_resamples(qda_wkflow, titanic_folds)
```

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*

#### Solution 6:
```{r}
# displays performance of logistic regression
collect_metrics(log_fit_resamples)
```
When considering which of the 3 fitted models have performed the best we want to look at the mean accuracy first, where we see that our `log_fit_resamples` has a mean accuracy of 0.8160798. Although this model's mean accuracy is not the "best", we see that its standard error of accuracy is very low with a `std_err` of 0.01731414. Therefore we should conclude that `log_fit_resamples` is our fitted model with the best performance.

### Question 7

Now that you’ve chosen a model, fit your chosen model to the entire training dataset (not to the folds).

#### Solution 7:
```{r}
# fits chosen model to entire training data
overall_fit <- fit(log_wkflow, titanic_train)
overall_fit
```

### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.

#### Solution 8:
```{r}
# tests model's accuracy
accuracies <- predict(overall_fit, titanic_test) %>%
  bind_cols(titanic_test$survived) %>%
  accuracy(truth = titanic_test$survived, estimate = .pred_class)
accuracies
```
As seen from our variable `accuracies` which tests our `overall_fit` accuracy of the logarithmic regression fit on the entire testing data (using the `predict()`, `bind_cols()` and `accuracy()` functions), we can see the average across all folds which gives us 0.7988827. This shows us that our model across all folds has an accuracy of approximately 79.88827%, which is very high and considerably accurate.	

# NOT IN PSTAT 231

## Required for 231 Students

Consider the following intercept-only model, with $\epsilon \sim N(0, \sigma^2)$:

$$
Y=\beta+\epsilon
$$

where $\beta$ is the parameter that we want to estimate. Suppose that we have $n$ observations of the response, i.e. $y_{1}, ..., y_{n}$, with uncorrelated errors.

### Question 9

Derive the least-squares estimate of $\beta$.

### Question 10

Suppose that we perform leave-one-out cross-validation (LOOCV). Recall that, in LOOCV, we divide the data into $n$ folds. What is the covariance between $\hat{\beta}^{(1)}$, or the least-squares estimator of $\beta$ that we obtain by taking the first fold as a training set, and $\hat{\beta}^{(2)}$, the least-squares estimator of $\beta$ that we obtain by taking the second fold as a training set?